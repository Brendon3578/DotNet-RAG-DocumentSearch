# DocumentSearchRagDotNetAI ‚Äì RAG com .NET + Ollama para Busca de Documentos

![Ollama](https://img.shields.io/badge/ollama-%23000000.svg?style=for-the-badge&logo=ollama&logoColor=white)
![C# .Net](https://img.shields.io/badge/c%23%20.NET-5C2D91?style=for-the-badge&logo=.net&logoColor=white)

## üìå Descri√ß√£o do Projeto

Este projeto √© uma **aplica√ß√£o de console em C#** criada para **testar e estudar a l√≥gica de RAG (Retrieval-Augmented Generation)** utilizando:

* **KernelMemory**
* **Ollama**
* **Modelos LLM locais**
* **Busca sem√¢ntica baseada em embeddings**

A aplica√ß√£o **simula a consulta a uma base de conhecimento corporativa**, como uma `biblioteca de documentos` internos de uma empresa (pol√≠ticas, normas, manuais, etc).

![Exweemplo de execu√ß√£o](./docs/exemplo-terminal.png)

---

## üß∞ Tecnologias Utilizadas

### Backend / CLI

* **Linguagem:** [C#](https://learn.microsoft.com/dotnet/csharp/)  
* **Framework:** [.NET](https://dotnet.microsoft.com/)  
* **Lib / Tool (RAG):** [Microsoft Kernel Memory](https://github.com/microsoft/kernel-memory)  
* **LLM / Runtime:** [Ollama](https://ollama.ai/)  
  * **Modelo de Gera√ß√£o de Texto (LLM):**
    * `deepseek-r1:8b` - [Modelo de LLM](https://ollama.com/library/deepseek-r1:8b)
  * **Modelo de Embeddings:**
    * `bge-m3` (1024 dimens√µes) - [Modelo de Gera√ß√£o de Embeddings](https://ollama.com/library/bge-m3)

### Fluxo simples do c√≥digo estruturado

```csharp
using Microsoft.KernelMemory;
using Microsoft.KernelMemory.AI.Ollama;
using Microsoft.KernelMemory.Configuration;

LogInfo("[SETUP] Iniciando aplica√ß√£o RAG (Retrieval-Augmented Generation) para consulta de base de documentos corporativa.");

// Configura√ß√£o do modelo de LLM (Large Language Model) para gera√ß√£o de texto e do modelo de Embeddings para vetoriza√ß√£o.
var config = new OllamaConfig
{
    Endpoint = "http://localhost:11434", // Endpoint local do servi√ßo Ollama
    TextModel = new OllamaModelConfig("deepseek-r1:8b", 131072), // Modelo LLM (DeepSeek). Contexto de 131k tokens permite processar prompts extensos.
    EmbeddingModel = new OllamaModelConfig("bge-m3", 1024) // Modelo de Embeddings atual (bge-m3). Gera vetores de 1024 dimens√µes para alta precis√£o sem√¢ntica.
};

// Inicializa a constru√ß√£o do KernelMemory integrando com Ollama.
// O KernelMemory abstrai a complexidade de ingest√£o, armazenamento vetorial e recupera√ß√£o de informa√ß√µes.
var memoryBuilder = new KernelMemoryBuilder()
    .WithOllamaTextGeneration(config)    // Configura o gerador de texto (LLM)
    .WithOllamaTextEmbeddingGeneration(config) // Configura o gerador de embeddings (Vetoriza√ß√£o)
    .WithCustomTextPartitioningOptions(new TextPartitioningOptions
    {
        // Configura√ß√£o de Chunking (Particionamento de texto) -> Divide os documentos em peda√ßos menores para indexa√ß√£o e busca vetorial.
        MaxTokensPerParagraph = 120, // Tamanho m√°ximo do chunk em tokens. Chunks menores focam em conceitos espec√≠ficos, melhorando a precis√£o da busca.
        OverlappingTokens = 30 // Sobreposi√ß√£o de tokens entre chunks adjacentes para preservar o contexto nas quebras de texto.
    });

var kernelMemory = memoryBuilder.Build();

LogInfo("Iniciando processo de ingest√£o e vetoriza√ß√£o de documentos...");

var documentsFiles = DocumentService.GetAllTxtDocumentsFromDirectoryPath("Files");

foreach (var documentFilePath in documentsFiles)
{
    await kernelMemory.ImportDocumentAsync(
        filePath: documentFilePath,
        documentId: documentFilePath,
        tags: new TagCollection
        {
            { "tipo", "politica" },
            { "departamento", "rh" },
            { "fonte", "interna" }
        });

    LogSuccess($"[INGEST] [SUCESSO] Documento indexado: '{documentFilePath}'");
}

LogInfo("[SETUP] Sistema RAG inicializado e pronto para processar consultas.");

while (true)
{
    LogInfo("[INTERFACE] Digite sua pergunta sobre as pol√≠ticas da empresa (ou 'sair' para encerrar):");

    var question = Console.ReadLine();

    // Prompt Engineering: Defini√ß√£o da "persona" e regras estritas para o modelo.
    // O objetivo √© evitar alucina√ß√µes (respostas fora do contexto) e manter o tom corporativo.
    var securePrompt = $"""
        Voc√™ √© um assistente corporativo.

        INSTRU√á√ïES OBRIGAT√ìRIAS:
        - Responda SOMENTE com base no CONTEXTO fornecido.
        - N√ÉO utilize conhecimento externo.
        - N√ÉO fa√ßa suposi√ß√µes.
        - Se n√£o houver informa√ß√£o suficiente, responda EXATAMENTE:
          "Desculpe, n√£o tenho essa informa√ß√£o no momento."

        FORMATO DA RESPOSTA:
        - Resposta curta e objetiva e no m√°ximo 5 linhas

        PERGUNTA:
        {question}
    """;

    LogInfo($"[RAG] Processando pergunta: \"{question}\"...");

    var response = await kernelMemory.AskAsync(
        securePrompt,
        filter: new MemoryFilter().ByTag("tipo", "politica").ByTag("departamento", "rh")
    );

    // Resposta do Prompt gerado pela LLM exibido no console
    LogAIResponse(response.Result);

    if (response.RelevantSources.Count > 0)
    {
        LogInfo("\n[RAG] --- Contexto Recuperado (RAG Retrieval) ---");

        var relevantSourceOrderedByRelevant = response.RelevantSources
            .OrderByDescending(source => source.Partitions.FirstOrDefault()?.Relevance)
            .ToList(); // Ordena as fontes pela relev√¢ncia do primeiro chunk encontrado

        foreach (var source in response.RelevantSources)
        {
            Log($"[SOURCE] ID: {source.DocumentId} | Arquivo: '{source.SourceName}' | Relev√¢ncia: {source.Partitions.FirstOrDefault()?.Relevance:f5}");
            
            foreach(var partition in source.Partitions)
            {
                 Log($"-- Trecho: {partition.Text}");
            }
        }
    }
};
```

‚ö†Ô∏è **Escopo intencionalmente simples**
Este projeto **n√£o √© uma API**, **n√£o √© produ√ß√£o**, e **n√£o possui interface gr√°fica**.
O foco √© **entender o fluxo completo de RAG** de ponta a ponta.

### Conceitos de IA e RAG

* **LLM (Large Language Model):** Modelo de linguagem utilizado para gerar respostas em linguagem natural.
* **Embeddings:** Representa√ß√µes vetoriais dos textos, utilizadas para busca sem√¢ntica.
* **RAG (Retrieval-Augmented Generation):** Arquitetura que combina recupera√ß√£o de contexto dos documentos com gera√ß√£o de resposta pela LLM.
* **Chunking:** Particionamento dos documentos em trechos menores para melhorar a precis√£o da busca e do contexto.

> [!TIP]
> N√£o h√° configura√ß√£o expl√≠cita de banco de dados relacional no c√≥digo; o armazenamento vetorial e de documentos √© gerenciado pela infraestrutura do Microsoft Kernel Memory.

---

## üîÑ Fluxo B√°sico do Projeto

```mermaid
sequenceDiagram
    participant User as Usuario
    participant Console as Console App
    participant KM as KernelMemory
    participant EMB as Embedding Model
    participant RET as Vector Search
    participant LLM as LLM Text Model

    User ->> Console: Digita pergunta
    Console ->> Console: Monta prompt seguro

    Console ->> KM: AskAsync com prompt e filtros
    KM ->> EMB: Gerar embedding da pergunta
    EMB -->> KM: Vetor da pergunta

    KM ->> RET: Buscar chunks similares
    RET -->> KM: Chunks relevantes

    KM ->> KM: Aplicar filtro por metadata
    KM ->> KM: Montar contexto com chunks

    KM ->> LLM: Enviar contexto e pergunta
    LLM -->> KM: Gerar resposta

    KM -->> Console: Resposta e fontes
    Console -->> User: Exibe resposta no console


```

**Resumo do fluxo:**

1. Usu√°rio inicia a aplica√ß√£o CLI e insere perguntas no console.  
2. A aplica√ß√£o j√° realizou previamente a **ingest√£o de documentos** em `Files/` usando o Kernel Memory.  
3. Em cada pergunta:
   * A pergunta √© enviada ao Kernel Memory, que executa o pipeline de **RAG**.
   * S√£o recuperados os **chunks mais relevantes** dos documentos de pol√≠ticas.
   * A LLM (via Ollama) gera uma resposta baseada nesse contexto.
4. A resposta e as principais fontes (documentId, relev√¢ncia etc.) s√£o exibidas no console.

---

## ‚ñ∂Ô∏è Como Executar o Projeto

### Pr√©-requisitos

* [.NET SDK](https://dotnet.microsoft.com/en-us/download) instalado (vers√£o compat√≠vel com o projeto).  
* [Ollama](https://ollama.ai/) instalado e em execu√ß√£o na m√°quina local.  
  * Endpoint padr√£o esperado pelo c√≥digo: `http://localhost:11434`  
  * Modelos utilizados (ou equivalentes compat√≠veis configurados no Ollama):
    * Modelo de LLM: `deepseek-r1:8b` (ou outro ajustado no c√≥digo).
    * Modelo de Embeddings: `bge-m3` (ou outro equivalente configurado no c√≥digo).
* Acesso aos arquivos `.txt` que ser√£o inseridos em `Files/` (por exemplo, pol√≠ticas corporativas).

> A escolha dos modelos foi baseada nos resultados obtidos durante os testes de avalia√ß√£o. O modelo de embeddings apresentou impacto significativo na acur√°cia da busca sem√¢ntica, resultando em respostas mais consistentes e alinhadas √†s perguntas realizadas.

### Passo a Passo

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/seu-usuario/DocumentSearchRagDotNetAI.git

cd DocumentSearchRagDotNetAI

# 2. (Opcional) Restaurar pacotes
dotnet restore

# 3. Execute o projeto
dotnet run

# 4. Garantir que o Ollama est√° rodando localmente em http://localhost:11434 com os models deepseek-r1:8b e bge-m3 baixados
# Exemplo (em outro terminal, se aplic√°vel):
ollama serve

# 5. Garantir que o diret√≥rio Files cont√©m os documentos .txt
# (por exemplo, pol√≠ticas de RH da empresa)

# 6. Executar a aplica√ß√£o
dotnet run
```

Ap√≥s a execu√ß√£o:

* A aplica√ß√£o realizar√° a **ingest√£o inicial** dos documentos em `Files/`.  
* Em seguida, o console exibir√° um prompt para voc√™ digitar perguntas sobre as pol√≠ticas da empresa.  
* Para sair, basta digitar `sair`.

---

## üê≥ (Opcional) Docker

Atualmente, o projeto **n√£o possui** um `Dockerfile` ou `docker-compose.yml` totalmente definido no reposit√≥rio.

Caso queira usar containers o projeto no futuro, um cen√°rio t√≠pico seria:

* **Dockerfile**:  
  * Respons√°vel por construir uma imagem contendo:
    * Runtime do .NET.
    * Artefatos da aplica√ß√£o.
    * Depend√™ncias necess√°rias para se comunicar com o Ollama.
* **Docker Compose** (opcional):  
  * Orquestrar m√∫ltiplos servi√ßos, por exemplo:
    * Um cont√™iner para a aplica√ß√£o .NET.
    * Um cont√™iner para o Ollama (caso seja suportado no ambiente).

Exemplo (futuro) de comando para subir o projeto com Docker Compose:

```bash
docker compose up --build
```

> Como essa infraestrutura ainda n√£o est√° definida no projeto, os passos acima servem apenas como refer√™ncia para uma futura implementa√ß√£o de Docker.

---

## üìå Observa√ß√µes Finais

* As respostas geradas pela LLM s√£o **estritamente condicionadas** ao contexto retornado pelo RAG:
  * Se n√£o houver informa√ß√£o suficiente, a aplica√ß√£o orienta a LLM a responder com uma mensagem padronizada indicando a aus√™ncia de dados.
* A **qualidade das respostas** depende diretamente:
  * Da qualidade dos documentos em `Files/`.
  * Da cobertura das pol√≠ticas e informa√ß√µes relevantes nesses arquivos.
* **Ambientes (dev/prod):**
  * Em desenvolvimento, √© comum:
    * Ajustar o modelo de LLM e de embeddings no c√≥digo.
    * Trabalhar com um conjunto menor de documentos para testes.
  * Em produ√ß√£o, recomenda-se:
    * Monitorar logs de relev√¢ncia m√©dia, fontes de contexto e tempo de resposta.
  * Manter os arquivos de pol√≠ticas atualizados no diret√≥rio de ingest√£o.
* Pr√≥ximos passos poss√≠veis:
  * Adicionar camadas de autentica√ß√£o/autoriza√ß√£o (se necess√°rio, em um futuro front-end ou API).
  * Implementar persist√™ncia expl√≠cita dos vetores em um reposit√≥rio pr√≥prio (caso o projeto cres√ßa).
  * Criar endpoints HTTP ou UI web para substituir/complementar o uso via console.

### üß† Observa√ß√£o sobre uso de containers

Em um cen√°rio de produ√ß√£o, o uso de Docker para a hospedagem de LLMs n√£o √© recomendado devido ao alt√≠ssimo consumo de recursos computacionais, especialmente CPU, GPU e mem√≥ria. Para ambientes produtivos, √© fortemente recomendado utilizar servi√ßos de IA gerenciados, que oferecem melhor escalabilidade, alta disponibilidade, monitoramento, controle de custos e atualiza√ß√µes cont√≠nuas dos modelos.

Exemplos de servi√ßos recomendados:

* Azure OpenAI Service, com integra√ß√£o nativa ao ecossistema .NET e recursos avan√ßados de - seguran√ßa.
OpenAI API, amplamente utilizada, com modelos de alta performance para gera√ß√£o de texto e - embeddings.
AWS Bedrock, que disponibiliza diversos modelos fundacionais com escalabilidade gerenciada.
* Google Vertex AI, com suporte a modelos generativos e pipelines de ML.
* Cohere, especializada em modelos de linguagem e embeddings para busca sem√¢ntica.
